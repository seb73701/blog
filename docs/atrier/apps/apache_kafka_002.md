---
title: Eviter le chaos dans un environnement KafkaÃ¯en
description: 
published: true
date: 2023-07-03T21:18:13.689Z
tags: 
editor: markdown
dateCreated: 2023-07-03T21:18:13.689Z
---

# Eviter le chaos dans un environnement KafkaÃ¯en

Non ne partez pas ! Je vous promets quâ€™Ã  la fin de cet article vous serez libre.  

Si vous comprenez comment fonctionne Apache Kafka, vous serez libre de vos choix et Ã©viterez ainsi le chaos.

La lÃ©gende raconte quâ€™Apache Kafka est nommÃ© ainsi par Jay Kreps (Ã  ce moment-lÃ  CEO de LinkedIn) car câ€™est un â€œsystÃ¨me optimisÃ© pour lâ€™Ã©critureâ€, Jay Kreps est Ã©galement un fervent lecteur de Franz Kafka. Nous pourrions approuver cette lÃ©gende sans se poser de questions si nous ne connaissions pas lâ€™Ã©crivain.

![crumb-kafka.jpg](../images/apps/apache_kafka/crumb-kafka.jpg)

Franz Kafka est un auteur moderne rÃ©putÃ© pour ses textes caractÃ©risÃ©s par une atmosphÃ¨re cauchemardesque et sinistre. Ses Å“uvres sont vues comme le symbole de l'homme dÃ©racinÃ© des temps modernes. Franz Kafka a averti sa premiÃ¨re fiancÃ©e, Felice Bauer, que si elle voulait partager le reste de sa vie avec lui, ce serait "une vie monastique, cÃ´te Ã  cÃ´te, avec un homme agitÃ©, mÃ©lancolique, silencieux, insatisfait et maladif".

Peut-Ãªtre Jay Kreps, supposait-il quelque chose en nommant la plateforme de streaming distribuÃ©e de cette maniÃ¨re ?

NÃ©anmoins, Franz Kafka, tout au long de sa vie, a su passionner les lecteurs et il est reconnu aujourdâ€™hui comme un Ã©crivain majeur du XXÃ¨me siÃ¨cle. Apache Kafka sâ€™est immiscÃ© au milieu des frameworks/plateformes Big Data et est devenu incontournable aujourdâ€™hui lorsque vous souhaitez gÃ©rer des Gigaoctets de donnÃ©es par seconde. Voyons quelques concepts et quelques maniÃ¨res dâ€™amÃ©liorer votre (futur) bus de messages prÃ©fÃ©rÃ©.

Si vous ne connaissez pas encore Apache Kafka, je vous redirige vers les articles existants du blog Ippon, vous trouverez de trÃ¨s bonnes explications sur son fonctionnement !

Gardez en tÃªte que ce nâ€™est â€œrien de plusâ€ quâ€™un commit log distribuÃ© sur plusieurs machines, tout cela grÃ¢ce au principe de partitionnement.

Nous allons voir plus en dÃ©tails comment configurer nos producteurs de messages et nos topics pour Ã©viter le chaos dans notre architecture distribuÃ©e.

## RÃ©plication
Au sein dâ€™un cluster Kafka se trouvent plusieurs brokers (en fait des serveurs) chargÃ©s de rÃ©partir uniformÃ©ment le nombre de partitions pour chaque topic, qui, eux, constituent une sÃ©quence ordonnÃ©e et immuable de messages.

Pour le bon fonctionnement de Kafka (et câ€™est toute la diffÃ©rence entre un systÃ¨me de queueing et de publish-subscribe), le systÃ¨me va garder chaque message sur le disque et lui appliquer une rÃ©tention rÃ©glÃ©e par dÃ©faut sur 168 heures soit 7 jours grÃ¢ce Ã  la propriÃ©tÃ© `log.retention.hours = 168`. Autrement dit, la donnÃ©e est persistÃ©e dans les disques de chaque broker et permet un trÃ¨s haut dÃ©bit en termes d'Ã©criture et de lecture.

Ã€ noter, qu'ici, Kafka utilise l'Ã©criture sur disque et non en RAM pour des raisons de coÃ»ts des composants (imaginez stocker des terra-octets en RAM...), mais Ã©galement car les Ã©critures sont optimisÃ©es grÃ¢ce au [principe de Zero-Copy](https://en.wikipedia.org/wiki/Zero-copy) et grÃ¢ce Ã  l'Ã©criture sÃ©quentielle ([sequential IO](https://docs.confluent.io/platform/current/kafka/design.html)).

Maintenant, imaginez votre cluster Kafka en production, il est superbe ! Vous avez mis 3 brokers, un topic avec un facteur de rÃ©plication de 3 (Ã  renseigner lors de la crÃ©ation du topic), rien ne peut vous arriver...

En fait si, imaginez que vous Ã©criviez dans un topic, et que le broker qui hÃ©berge la partition leader (â‰  followers) tombe juste aprÃ¨s avoir reÃ§u votre acquittement (ACK) : la donnÃ©e, peut ne pas avoir Ã©tÃ© encore rÃ©pliquÃ©e sur les autres brokers.

Dans un environnement Ã  haut dÃ©bit, critique et temps rÃ©el (banking/trading etc...), cela peut Ãªtre problÃ©matique de perdre cette prÃ©cieuse donnÃ©e !


Du coup, dans ce cas-lÃ , on aimerait plutÃ´t recevoir un non-acquittement (NACK) de la part du systÃ¨me non ? Histoire d'avoir un systÃ¨me cohÃ©rent et pouvoir gÃ©rer au cas par cas ces problÃ¨mes.

## Acquittement
Kafka est une plateforme optimisÃ©e pour la faible latence et le haut dÃ©bit, par dÃ©faut, l'acquittement (ACK) est configurÃ© pour n'Ãªtre envoyÃ© que lorsque le leader reÃ§oit et traite la donnÃ©e (comme le cas ci-dessus) grÃ¢ce Ã  la propriÃ©tÃ© `acks = 1`

Si vous dÃ©cidez, par excÃ¨s de folie, de mettre la propriÃ©tÃ© `acks = 0`, aucune garantie ne peut Ãªtre faite que le broker a reÃ§u le message. Si un problÃ¨me survient, les messages, une fois envoyÃ©s, ne seront pas rÃ©Ã©mis. Ce paramÃ¨tre offre une latence plus faible et un dÃ©bit plus Ã©levÃ© au prix d'un risque beaucoup plus Ã©levÃ© de perte de message.

Afin d'assurer la durabilitÃ© de la donnÃ©e au sein de notre cluster, il est prÃ©fÃ©rable de configurer la propriÃ©tÃ© comme suit acks=all. Ainsi, une fois que le leader reÃ§oit les acquittements de tous les autres brokers, indiquant qu'ils ont rÃ©pliquÃ© le message, il renverra un acquittement au producteur. Cela garantit que l'enregistrement ne sera pas perdu tant qu'au moins un rÃ©plica synchronisÃ© reste actif.

> ğŸ‘¨â€ğŸ’» Reprenons notre cluster en production, j'envoie un message dans mon topic, ma donnÃ©e va Ãªtre rÃ©pliquÃ©e sur les 3 brokers, mais il ne m'enverra un acquittement qu'une fois la donnÃ©e rÃ©pliquÃ©e partout ! Ã‡a devrait couvrir notre cas ci-dessus ?



Oui, mais Ã§a ne suffit pas...

Il existe une notion d'ISR (in-sync replica) qui reprÃ©sente tous les rÃ©plicas d'une partition qui sont "synchronisÃ©s" avec le leader. Au minimum, l'ISR comprendra le rÃ©plica leader et tous les rÃ©plicas followers supplÃ©mentaires qui sont Ã©galement considÃ©rÃ©s comme synchronisÃ©s. Les followers rÃ©pliquent les donnÃ©es du leader en envoyant des demandes de synchronisations pÃ©riodiques, par dÃ©faut l'ISR est positionnÃ© sur 1 grÃ¢ce Ã  la propriÃ©tÃ© `min.insync.replicas=1` et ceci toujours dans un but d'avoir un systÃ¨me Ã  faible latence et Ã  haut dÃ©bit, car en augmentant le nombre d'ISR, chacun des brokers va devoir se synchroniser pour vÃ©rifier la rÃ©plication du message.

Dans notre cas, nous appliquerons une valeur de 2 Ã  la propriÃ©tÃ© `min.insync.replicas` ce qui veut dire que si 1 rÃ©plica tombe parmi les 3, grÃ¢ce Ã  notre replication factor de 3 (chaque broker hÃ©berge une copie de chaque partition du topic) et grÃ¢ce Ã  `acks=all`, on pourra enfin recevoir un acquittement (ACK) qui validera les rÃ¨gles minimales de haute disponibilitÃ© que nous nous sommes donnÃ©es.

Si, par malchance, 2 de vos brokers tombent pendant l'envoi de votre message dans le topic, alors un non-acquittement (NACK) accompagnÃ© d'une exception `NotEnoughReplicas`, vous indiquera que votre message n'a pas pu Ãªtre dÃ©livrÃ© et rÃ©pliquÃ© sur nos brokers.

> ğŸ‘¨â€ğŸ’» Bon lÃ  j'ai un super systÃ¨me haute disponibilitÃ©, faible latence et haut dÃ©bit ! On part en prod ?

Pas si vite...

## StratÃ©gie de retry
Dans la plateforme Kafka, par dÃ©faut, il existe un systÃ¨me de retry qui peut vous Ãªtre trÃ¨s trÃ¨s utile !

Avant la version 2.1, le systÃ¨me existait mais Ã©tait dÃ©sactivÃ© par dÃ©faut. Depuis, cette version, le systÃ¨me de retry est configurÃ© sur la valeur `Integer.MAX_VALUE` (2147483647) via la propriÃ©tÃ© `retries=2147483647` dans votre producteur et est donc activÃ© par dÃ©faut.

Une erreur "retryable" est une erreur transitoire :

- de type TimeoutException,
- de sockets,
- RÃ©Ã©lection d'un leader pour le topic.

Dans les pannes Ã©voquÃ©es ci-dessus, il vous sera utile d'utiliser la stratÃ©gie de retry (Ã  condition qu'elle soit activÃ©e via la propriÃ©tÃ© ci-dessus...) afin de ne pas perdre votre donnÃ©e.

> ğŸ‘¨â€ğŸ’» Je suis en 2.1, donc je n'ai rien Ã  faire, ils sont forts les dÃ©veloppeurs de Kafka, ils ont pensÃ© Ã  tout !

En effet, ce levier va nous permettre de garantir la durabilitÃ© de la donnÃ©e dans notre pipeline.

Mais il y a un mais... (encore ?????)

L'ordre n'est plus garanti et l'unicitÃ© non plus. ğŸ˜©

## Idempotence
Imaginez cette fois-ci que vous envoyiez un message dans votre topic, tout se passe bien, il est rÃ©pliquÃ©... Mais lorsque le broker va vouloir envoyer l'acquittement (ACK), un Timeout a lieu. Votre donnÃ©e est dans le cluster, mais le producteur, ayant reÃ§u un non-acquittement, va rÃ©essayer d'envoyer le message. Et cette fois-ci, Ã§a fonctionne.

On se retrouve ainsi avec notre donnÃ©e en double dans notre cluster ğŸ¤¯


Pour corriger ceci, nous allons pouvoir activer la propriÃ©tÃ© `enable.idempotence=true` dans le producteur.

Le producteur va s'assurer qu'exactement une seule copie du message existe dans le topic (en injectant un UUID dans le message, qui permet au producteur de vÃ©rifier s'il a dÃ©jÃ  Ã©tÃ© traitÃ© ou pas, que Ã§a soit sur les partitions leaders ou followers).

> ğŸ‘¨â€ğŸ’» On peut partir en prod plus sereinement !

## Conclusion
Kafka est une plateforme avec des concepts clÃ©s et une trÃ¨s grande flexibilitÃ© de configuration.

Retenez bien que la plateforme est conÃ§ue, par dÃ©faut, pour avoir une latence faible et un haut dÃ©bit de production/consommation.

Tous les systÃ¨mes distribuÃ©s doivent faire des compromis entre garantir la cohÃ©rence, la disponibilitÃ© et la tolÃ©rance au partitionnement rÃ©seau ([thÃ©orÃ¨me CAP](http://en.wikipedia.org/wiki/CAP_theorem)) :

CohÃ©rence : chaque lecture reÃ§oit l'Ã©criture la plus rÃ©cente ou une erreur,
DisponibilitÃ© : chaque lecture reÃ§oit une rÃ©ponse sans erreur, mÃªme s'il ne s'agit pas de l'Ã©criture la plus rÃ©cente,
TolÃ©rance au partitionnement rÃ©seau : le systÃ¨me continue de fonctionner mÃªme si une partie du rÃ©seau tombe en panne.
D'aprÃ¨s les [ingÃ©nieurs de LinkedIn](https://engineering.linkedin.com/kafka/intra-cluster-replication-apache-kafka) Ã  l'origine du projet Kafka, la plateforme a Ã©tÃ© conÃ§ue dans le but de garantir la haute disponibilitÃ© et la cohÃ©rence des donnÃ©es, plutÃ´t que la tolÃ©rance au partitionnement rÃ©seau en prÃ©sumant qu'ils arrivent rarement au sein d'un datacenter.

GrÃ¢ce Ã  la configuration fine de Kafka, vous allez Ãªtre en capacitÃ© de pouvoir dÃ©ployer des clusters hautes-performances comme le font [Criteo en Europe](https://fr.slideshare.net/RicardoPaiva17/how-criteo-is-managing-one-of-the-largest-kafka-infrastructure-in-europe) ou [LinkedIn aux USA](https://engineering.linkedin.com/blog/2019/apache-kafka-trillion-messages).

N'hÃ©sitez pas Ã  consulter la [riche documentation](https://kafka.apache.org/documentation/) d'Apache Kafka si vous souhaitez aller encore plus loin. ğŸ˜

source : https://blog.ippon.fr/